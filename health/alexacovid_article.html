<html>
    <head>
        <title>Alexa, do I have COVID-19?</title>
        <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" />
        <link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" />
        <!-- style-->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <!--<script type="text/javascript" src="../2040/2040.js"></script>-->
        
        <meta name="DC.title" lang="en" content="Alexa, do I have COVID-19?" />
        <meta name="DC.creator" content="Emily Anthes" />
        <meta name="DCTERMS.issued" scheme="DCTERMS.W3CDTF" content="2020-09-30" />
        <meta name="DC.publisher" content="Nature" />
        <meta name="DC.identifier" scheme="DCTERMS.URI" content="https://www.nature.com/articles/d41586-020-02732-4" />
        <meta name="DC.format" scheme="DCTERMS.IMT" content="text/html" />
        <meta name="DC.type" scheme="DCTERMS.DCMIType" content="Text" />
        <meta name="DCTERMS.bibliographicCitation" content="doi: 10.1038/d41586-020-02732-4, 2020/09/30" />
        <meta name="keywords" content="Alexa, covid-19, dementia, depression"/>
  </head>
  <body>
    <!-- Article -->
  	<div id="ArticleBody" class="article">
  		<div id="CoverWrapper" class="cover">
  			<div class="maincover">
                <div class="parallax"><img src="../2040/img/Eye 8.png" class="logo"></div>
                <div class="coverpic">
                    <figure>
                      <img src="alexa.jpg" class="coverimg">
                     <figcaption class="covercaption">Illustration by Rune Fisker</figcaption>
                    </figure> 
                </div>  
                <div class="title">
	    			<h1>Alexa, do I have COVID-19?</h1>
	     			<p class="subtitle">Researchers are exploring ways to use people’s voices to diagnose coronavirus infections, dementia, depression and much more.</p>
                </div> 
                <div class="coverinfo">
                    <p class="byline">By <span class="person">Emily Anthes</span></p>
                    <p class="publicationDate">2020-09-30</p>
                    <div id="readingTime"></div>
                    <div id="listeningTime"></div>
                    <div id="icons">
                        <i id="reading" class="fa fa-file-text-o" aria-hidden="true" style="font-size: 200%; padding:10%;" ;></i>
                        <i id="listening" class="fa fa-microphone" aria-hidden="true" style="font-size: 200%;  padding:10%;";></i>
                    </div>
                 </div>  
  				
	    	</div>	         
          </div>
          <div id="ListeningWrapper">
            <div id="audiobox">
                <button id="play-btn">
                    <i class="fa fa-pause" aria-hidden="true" style="font-size: 200%; padding:10%;"></i>
                </button>
            </div>
        </div>
  		<div id="ContentWrapper" class="content">
            <p> 
                In March, as the staggering scope of the coronavirus pandemic started to become clear, officials around the world began enlisting the public to join in the fight. Hospitals asked local companies to donate face masks. 
                Researchers called on people who had recovered from <span class="concept">COVID-19</span> to donate their blood plasma. 
                And in <span class="place">Israel</span>, the defence ministry and a start-up company called <span class="entity">Vocalis Health</span> asked people to donate their voices.
            </p>
            <p>
                <span class="entity">Vocalis</span>, a voice-analysis company with offices in <span class="place">Israel</span> and the <span class="place">United States</span>, had previously built a smartphone app that could detect flare-ups of chronic obstructive pulmonary disease by listening for signs that users were short of breath when speaking. 
                The firm wanted to do the same thing with <span class="concept">COVID-19</span>. People who had tested positive for the coronavirus could participate simply by downloading a <span class="entity">Vocalis</span> research app. Once a day, they fired up the app and spoke into their phones, describing an image aloud and counting from 50 to 70.
            </p>
            <p>
                Then <span class="entity">Vocalis</span> began processing these recordings with its <span class="concept">machine-learning system</span>, alongside the voices of people who had tested negative for the disease, in an attempt to identify a voiceprint for the illness. By mid-summer, the firm had more than 1,500 voice samples and a pilot version of a digital <span class="concept">COVID-19</span> screening tool. 
                The tool, which the company is currently testing around the world, is not intended to provide a definitive diagnosis, but to help clinicians triage potential cases, identifying people who might be most in need of testing, quarantine or in-person medical care. “Can we help with our <span class="concept">AI</span> algorithm?” asks <span class="person">Tal Wenderow</span>, the president and chief executive of <span class="entity">Vocalis</span>. 
                “This is not invasive, it’s not a drug, we’re not changing anything. All you need to do is speak.”
            </p>
            <p>
                They’re not the only ones racing to find vocal biomarkers of <span class="concept">COVID-19</span> — at least three other research groups are working on similar projects. Other teams are analysing audio recordings of <span class="concept">COVID-19</span> coughs and developing voice-analysis algorithms designed to detect when someone is wearing a face mask.
            </p>
            <p>
                It’s a sign of how hungry the young field of vocal diagnostics is to make its mark. Over the past decade, scientists have used artificial intelligence (<span class="concept">AI</span>) and <span class="concept">machine-learning system</span> to identify potential vocal biomarkers of a wide variety of conditions, including <span class="concept">dementia</span>, <span class="concept">depression</span>, <span class="concept">autism</span> spectrum disorder and even heart disease. 
                The technologies they have developed are capable of picking out subtle differences in how people with certain conditions speak, and companies around the world are beginning to commercialize them.
            </p>
            <p>
                For now, most teams are taking a slow, stepwise approach, designing tailored tools for use in doctors’ offices or clinical trials. But many dream of deploying this technology more widely, harnessing microphones that are ubiquitous in consumer products to identify diseases and disorders. 
                These systems could one day allow epidemiologists to use smartphones to track the spread of disease, and turn smart speakers into in-home medical devices. 
                “In the future, your robot, your <span class="concept">Siri</span>, your <span class="concept">Alexa</span> will simply say, ‘Oh you’ve got a cold,’” says <span class="person">Björn Schuller</span>, a specialist in speech and emotion recognition with a joint position at the <span class="entity">University of Augsburg</span> in <span class="place">Germany</span> and <span class="entity">Imperial College of London</span>, who is leading one of the <span class="concept">COVID-19</span> studies.
            </p>
            <p>
                But automated vocal analysis is still a new field, and has a number of potential pitfalls, from erroneous diagnoses to the invasion of personal and medical privacy. Many studies remain small and preliminary, and moving from proof-of-concept to product won’t be easy. 
                “We are at the early hour of this,” <span class="person">Schuller</span> says.
            </p>
            <p>
                Some ailments cause obvious vocal distortions; consider the telltale stuffiness of someone afflicted by allergies. But many scientists think that vocal analysis could help to identify an enormous range of disorders, thanks to the complexity of human speech.
            </p>

            <h2 id="section1">Speech signals</h2>
            <p>
                Speaking requires the coordination of numerous anatomical structures and systems. The lungs send air through the vocal cords, which produce sounds that are shaped by the tongue, lips and nasal cavities, among other structures. 
                The brain, along with other parts of the nervous system, helps to regulate all these processes and determine the words someone is saying. 
                A disease that affects any one of these systems might leave diagnostic clues in a person’s speech.
            </p>
            <p>
                <span class="concept">Machine learning</span> has given scientists a way to detect aberrations, quickly and at scale. 
                Researchers can now feed hundreds or thousands of voice samples into a computer to search for features that distinguish people with various medical conditions from those without them.
            </p>
            <p>
                Much of the early work in the field focused on <span class="concept">Parkinson</span>’s disease, which has well-known effects on speech — and for which there is no definitive diagnostic test. The disorder causes a variety of motor symptoms, including tremors, muscle stiffness and problems with balance and coordination. 
                The loss of control extends to the muscles involved in speech; as a result, many people with  <span class="concept">Parkinson</span>’s have weak, soft voices. “It’s one of those things that you can hear with the human ear,” says <span class="person">Reza Hosseini Ghomi</span>, a neuropsychiatrist at <span class="entity">EvergreenHealth</span> in <span class="place">Kirkland</span>, <span class="place">Washington</span>, who has identified <span class="concept">vocal features</span> associated with several neurodegenerative diseases. 
                “But if you can get 10,000 samples and a computer, you can get much more accurate.”
            </p>
            <p>
                More than a decade ago, <span class="person">Max Little</span>, a researcher in <span class="concept">machine learning</span> and signal processing now at the <span class="entity">University of Birmingham</span>, <span class="place">UK</span>, began to investigate whether voice analysis might help doctors to make difficult diagnoses. 
                In one study, <span class="person">Little</span> and his colleagues used audio recordings of 43 adults, 33 of whom had <span class="concept">Parkinson</span>’s disease, saying the syllable “ahhh”. They used speech-processing algorithms to analyse 132 <span class="concept">acoustic features</span> of each recording, ultimately identifying 10 — including characteristics such as breathiness and tremulous oscillations in pitch and timbre — that seemed to be most predictive of <span class="concept">Parkinson</span>’s. Using just these 10 features, the system could identify the speech samples that came from people with the disease with nearly 99% accuracy<sup>1</sup>.
            </p>
            <p>
                <span class="person">Little</span> and others in the field have also demonstrated that certain <span class="concept">vocal features</span> correlate with the severity of <span class="concept">Parkinson</span>’s symptoms. The systems are not yet robust enough for routine use in clinical practice, <span class="person">Little</span> says, but there are many potential applications. 
                Vocal analysis might provide a quick, low-cost way to monitor individuals who are at high risk of the disease; to screen large populations; or possibly even to create a telephone service that could remotely diagnose people who don’t have access to a neurologist. 
                Patients could use the technology at home — in the form of a smartphone app, say — to track their own symptoms and monitor their response to medication. “This kind of technology can allow a high-speed snapshot, an almost continuous snapshot of how someone’s symptoms are changing,” <span class="person">Little</span> says.
            </p>

            <figure>
				<img src="parkinson.jpg">
				<figcaption>A man with <span class="concept">Parkinson</span>’s disease works on his speech with his wife. Vocal changes associated with the disorder could help doctors to diagnose it and assess treatments.Credit: <span class="person">Don Kelsen</span>/Los Angeles Times/Getty</figcaption>
            </figure>
            
  			<p>
                Researchers are now working to identify speech-based biomarkers for other kinds of neurodegenerative disease. For instance, a trio of scientists in <span class="place">Toronto</span>, <span class="place">Canada</span>, used voice samples and transcripts from more than 250 people to identify dozens of differences between the speech of people with possible or probable <span class="concept">Alzheimer</span>’s disease and that of people without it<sup>2</sup>. 
                Among the participants, those with <span class="concept">Alzheimer</span>’s tended to use shorter words, smaller vocabularies and more sentence fragments. They also repeated themselves and used a higher ratio of pronouns, such as ‘it’ or ‘this’, to proper nouns. “It can be a sign that they’re just not remembering the names of things so they have to use pronouns instead,” says <span class="person">Frank Rudzicz</span>, a computer scientist at the <span class="entity">University of Toronto</span>, who led the study.
            </p>
            <p>
                When the system considered 35 of these vocal features together, it was able to identify people with <span class="concept">Alzheimer</span>’s with 82% accuracy. (This has since improved to roughly 92%, <span class="person">Rudzicz</span> says, noting that the errors tend to be more or less evenly split between false negatives and false positives.) “Those features add up to sort of a fingerprint of dementia,” <span class="person">Rudzicz</span> says. 
                “It’s a very intricate hidden pattern that’s hard for us to see on the surface, but <span class="concept">machine-learning</span> can pick it out, given enough data.”
            </p>
            <p>
                Because some of these vocal changes occur in the early stages of neurodegenerative diseases, researchers hope that voice-analysis tools could eventually help clinicians to diagnose such conditions earlier and potentially intervene before other symptoms become obvious.
            </p>
            <p>
                For now, however, this idea remains largely theoretical; scientists still need to do large, long-term, longitudinal trials to demonstrate that voice analysis can actually detect disease earlier than standard diagnostic methods can.
            </p>
            <p>
                And some clinicians note that voice analysis alone will rarely yield definitive diagnoses. “I learn a lot by listening to someone’s voice,” says <span class="person">Norman Hogikyan</span>, a laryngologist at the <span class="entity">University of Michigan</span> in <span class="place">Ann Arbor</span>. “I do it for a living. But I put it together with a history and then my exam. All three parts of that assessment are important.”
            </p>
            <p>
                Researchers in the field stress that the goal is not to replace doctors or create standalone diagnostic devices. Instead, they see voice analysis as a tool physicians can use to inform their decisions, as another ‘vital sign’ they can monitor or test they can order. “My vision is that collecting speech samples will become as common as a blood test,” says <span class="person">Isabel Trancoso</span>, a researcher in spoken-language processing at the <span class="entity">University of Lisbon</span>.
            </p>

            <h2 id="section3">Expanding applications</h2>
            <p>
                A number of voice-analysis start-ups — including <span class="entity">Winterlight Labs</span>, a Toronto firm co-founded by <span class="person">Rudzicz</span>, and <span class="entity">Aural Analytics</span> in <span class="place">Scottsdale</span>, <span class="place">Arizona</span> — are now providing their software to pharmaceutical companies. Many are using the technology to help assess whether people enrolled in their clinical trials are responding to experimental treatments. 
                “Using speech as a more subtle proxy for changes in neurological health, you can help push drugs across the finish line or at the very least identify those that are not promising early on,” says <span class="person">Visar Berisha</span>, the co-founder and chief analytics officer at <span class="concept">Aural Analytics</span>.
            </p>
            <p>
                Neurodegenerative disorders are just the beginning. Scientists have identified distinct speech patterns in children with neurodevelopmental disorders. In one small 2017 study, <span class="person">Schuller</span> and his colleagues determined that algorithms that analysed the babbling of ten-month-old infants could identify with some accuracy which children would go on to be diagnosed with autism spectrum disorder<sup>3</sup>. 
                The system correctly classified roughly 80% of children with autism and 70% of neurotypical children.
            </p>
            <p>
                Researchers have also found that many children with attention deficit hyperactivity disorder speak louder and faster than their neurotypical peers, and show more signs of vocal strain. The firm <span class="entity">PeakProfiling</span> in <span class="place">Berlin</span> is now developing a clinical speech-analysis tool that it hopes can help doctors to diagnose the condition.
            </p>
            <p>
                But some clinicians are sceptical about how much useful information such systems will really provide. “Some of it is a little overblown,” says <span class="person">Rhea Paul</span>, a specialist in communication disorders at <span class="entity">Sacred Heart University</span> in <span class="place">Fairfield</span>, <span class="place">Connecticut</span>. Children with neurodevelopmental disorders often have many easily observable behavioural symptoms, she notes.
            </p>
            <p>
                Moreover, it’s not yet clear whether the algorithms are really identifying specific markers for, say, autism spectrum disorder, or just picking up on general signs of atypical brain development — or even just transient aberrations in speech. “Development is a meandering path and not every kid who starts out looking like they have <span class="concept">autism</span> grows up to be an adult with autism,” <span class="person">Paul</span> says. 
                Even if scientists do identify a highly reliable, specific vocal biomarker, she adds, it should only be used to identify children who might benefit from a more thorough evaluation. “It shouldn’t be sufficient in and of itself to label a child, especially so early in life.”
            </p>
            <p>
                Scientists are also turning the technology to <span class="concept">mental illnesses</span>. Numerous teams around the world have developed systems that can pick up on the slow, pause-heavy, monotonous speech that tends to characterize depression, and others have identified vocal biomarkers associated with psychosis, suicidality and bipolar disorder.
            </p>
            <p>
                “<span class="concept">Voice</span> is enormously rich in terms of carrying our emotion signals,” says <span class="person">Charles Marmar</span>, a psychiatrist at <span class="entity">New York University</span>. “The rate, the rhythm, the volume, the pitch, the prosody [stress and intonation] — those features, they tell you whether a patient is down and discouraged, whether they’re agitated and anxious, or whether they’re dysphoric and manic.”
            </p>
            <p>
                In his own work, <span class="person">Marmar</span> has used machine learning to identify 18 vocal features associated with post-traumatic stress disorder (<span class="concept">PTSD</span>) in 129 male military veterans. By analysing these features — which were mainly indicators of slow, flat, monotonous speech — the system could identify, with nearly 90% accuracy, which of the veterans had PTSD<sup>4</sup>.
            </p>
            <p>
                <span class="person">Marmar</span> and his colleagues are now expanding their research to women and civilians; if the team can generalize the findings, <span class="person">Marmar</span> thinks that the technology could be a useful way to quickly identify people who might need a more thorough psychiatric assessment. “The first real-world application would be for high-throughput screening of PTSD,” he says. “You can do 4,000 voice screens in a matter of hours.”
            </p>
            <p>
                Similar consumer applications are already beginning to make their way into the world. The <span class="concept">US Department of Veterans Affairs</span> is studying whether an app that monitors mental health can identify service members experiencing psychological distress. 
                The smartphone app, developed by <span class="entity">Cogito</span>, a conversational guidance and analytics company in <span class="place">Boston</span>, <span class="place">Massachusetts</span>, collects metadata on users’ habits — such as how frequently they call or text other people — and analyses voice memos they leave on their phones.
            </p>
            <p>
                There might even be vocal biomarkers for conditions that seem to have nothing to do with speech. In one study from 2018, scientists analysing speech samples from 101 people who were scheduled to undergo coronary angiograms discovered that certain vocal frequency patterns were associated with more severe coronary artery disease<sup>5</sup>.
            </p>
            <p>
                It’s not clear what explains these differences. “We struggle with the mechanism because it’s not obvious,” says <span class="person">Amir Lerman</span>, a cardiologist at the Mayo Clinic in <span class="place">Rochester</span>, <span class="place">Minnesota</span>, who led the research. Coronary artery disease could theoretically change the voice by reducing blood flow, he says. But it’s also possible that it’s not the disease itself that causes the vocal changes, but other associated risk factors, such as stress or depression.
            </p>

            <h2 id="section3">Tricky translation</h2>
            <p>
                That study demonstrates both the promise and the limitations of this technology. It’s one thing for a computer to pick out vocal patterns, but it’s another, harder task to understand what they mean and whether they’re clinically meaningful. Are they fundamental characteristics of the disease in question? Or merely markers of some other difference between groups, such as age, sex, body size, education or fatigue, any of which could be a confounding factor? 
                “We’re trying to move away from just shoving data into an algorithm, and really diving into the data sets, coming up with a model of the disease first and then testing that with machine learning,” <span class="person">Ghomi</span> says.
            </p>
            <p>
                Most studies so far have identified potential biomarkers in just a small, single population of patients. “Reproducibility is still a question,” <span class="person">Lerman</span> says. “Is my voice today and tomorrow and the day after tomorrow the same?” To ensure that the results can be generalized — and to reduce the possibility of bias, a problem known to plague medical algorithms — researchers will need to test their classification systems in larger, more diverse samples and in a variety of languages. 
                “We don’t want to validate a speech model just with 300 patients,” says <span class="person">Jim Schwoebel</span>, vice-president of data and research at Sonde Health, a Boston-based voice-analysis company. “We think we need 10,000 or more.”
            </p>
            <p>
                The company runs SurveyLex, an online platform that allows researchers to easily create and distribute voice surveys, as well as the Voiceome project, an effort to collect voice samples and health information from up to 100,000 people, across a wide variety of speech tasks, locations and accents. “You might be depressed in <span class="place">New York</span>, and sound differently depressed in <span class="place">Houston</span>, <span class="place">Texas</span>,” <span class="person">Schwoebel</span> says.
            </p>
            <p>
                For many of the applications that researchers have in mind, voice-analysis systems will have to not only distinguish sick people from healthy controls, but also differentiate between a variety of illnesses and conditions. And they’ll need to do this outside the lab, in uncontrolled everyday situations and on a wide variety of consumer devices. 
                “You’ve got smartphones which have a limited range of sensors, and people are using them everywhere in very uncontrolled environments,” says <span class="person">Julien Epps</span>, a researcher who studies speech-signal processing at the University of New South Wales in Sydney, Australia.
            </p>
            <p>
                When Epps and his colleagues, including a researcher at Sonde Health, analysed voice samples recorded with high-quality microphones in a lab, they were able to detect depression with roughly 94% accuracy (see ‘Depressed tones’). When using speech samples that people recorded in their own environments on their smartphones, the accuracy dropped to less than 75%, the researchers reported in a 2019 paper6.
            </p>

            <figure>
				<img src="tones_graphic.png">
				<figcaption>Source: Zhaocheng Huang, Univ. New South Wales</figcaption>
			</figure>

            <p>
                And just because the technology is non-invasive doesn’t mean that it is without risks. It poses serious privacy concerns, including the possibility that individuals could be identified from anonymous speech samples, that the systems might inadvertently capture private conversations, and that sensitive medical information could be sold, shared, hacked or misused.
            </p>
            <p>
                If the technology isn’t regulated properly, there’s a danger that insurers or employers could use these systems to analyse speech samples without explicit consent or to obtain personal health information, and potentially discriminate against their customers or employees.
            </p>
            <p>
                And then there’s the perennial risk of false positives and overdiagnosis. “We have to be real and realize that a lot of this is still research,” <span class="person">Rudzicz</span> says. “And we need to start thinking about what’s going to happen when we put it into practice.”
            </p>

            <section id="References">
                <h2>References</h2> <!-- capire se mettere i link come nella pagina originale -->
                <p class="biblioItem" id="b01"><span class="biblioMarker">1. </span>Tsanas, T., Little, M. A., McSharry, P. E., Spielman, J. & Ramig, L. O. IEEE Trans Biomed. Eng. 59, 1264–1271 (2012).</p>
                <p class="biblioItem" id="b02"><span class="biblioMarker">2. </span>Fraser, K. C., Meltzer, J. A. & Rudzicz, F. J. Alzheimers Dis. 49, 407–422 (2016).</p>
                <p class="biblioItem" id="b03"><span class="biblioMarker">3. </span>Pokorny, F. B. et al. Proc. 18th Annu. Conf. Int. Speech Commun. Assoc. (INTERSPEECH) 2017, 309–313 (2017).</p>
                <p class="biblioItem" id="b04"><span class="biblioMarker">4. </span>Marmar, C. R. et al. Depress. Anxiety 36, 607–616 (2019).</p>
                <p class="biblioItem" id="b05"><span class="biblioMarker">5. </span>Maor, E. et al. Mayo Clin. Proc. 93, 840–847 (2018).</p>
                <p class="biblioItem" id="b06"><span class="biblioMarker">6. </span>Huang, Z., Epps, J. & Joachim, D. IEEE Trans. Affect. Comput. https://doi.org/10.1109/taffc.2019.2944380 (2019).</p>
            </section>
            <div id="FootWrapper" class="foot"> <!--chiedere se questo va bene qui anche se nell'articolo è prima delle references-->
                <p class="copy">Nature 586, 22-25 (2020)</p>
                <p class="pubnote">doi: 10.1038/d41586-020-02732-4</p>
            </div>
            <div id="Footerimg"><img src="../2040/img/sust_3.png" class="footerimg"></div>
        </div>
        <script>
            
            

            var status = null;
            responsiveVoice.cancel();
            $('#listening').click(function() {
                if (status == null) {
                responsiveVoice.speak(document.getElementById("ContentWrapper").textContent);
                console.log(document.getElementById("ContentWrapper").textContent)
                status = "play";
                }
            });

            $('#play-btn').click(function() {
                if (status == "play") {
                responsiveVoice.pause();
                status="pause";
                document.getElementById("play-btn").innerHTML = '<i class="fa fa-play" aria-hidden="true" style="font-size: 200%; padding:10%;"></i>';
                }
                if (status ="pause") {
                responsiveVoice.resume();
                status = "play";
                document.getElementById("play-btn").innerHTML = '<i class="fa fa-pause" aria-hidden="true" style="font-size: 200%; padding:10%;"></i>' ;
                }
            });
        </script>
    </body>
</html>